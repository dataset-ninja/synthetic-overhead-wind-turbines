The authors of the dataset present a set of synthetic overhead imagery of wind turbines, which was generated using CityEngine. The synthetic images were designed to qualitatively match real overhead images of wind turbines from California and Arizona. Each image in the dataset is accompanied by corresponding labels that provide important information, such as the class, x and y coordinates, and height and width of the ground truth bounding boxes for each wind turbine.

The main purpose of this dataset is to serve as a supplement for training object detection models on overhead images of wind turbines. By adding this synthetic data to the training set, it is intended to potentially enhance the model's performance, especially when dealing with small wind turbines or images captured in desert regions.

The dataset was created with the objective of studying the use of synthetic imagery for cross-domain testing. During the training of the YOLOv3 model on the "Overhead Imagery of Wind Turbines" dataset, the authors observed that the model faced challenges when dealing with images from California and Arizona, where smaller wind turbines were present. Due to the size of these turbines, there was less information available for the model to correctly identify them. Often, the only noticeable information was the shadows of the turbines. To address this issue, the authors designed the synthetic imagery to potentially improve the model's performance in these specific regions and with this type of wind turbine. The synthetic images were added to the training set, and the model's performance was re-evaluated on the testing set consisting of data from California and Arizona.

To create the dataset, the authors selected background images from a publicly available dataset called [Power Plant Satellite Imagery Dataset](https://figshare.com/articles/dataset/Power_Plant_Satellite_Imagery_Dataset/5307364). The selected background images were chosen to avoid scenes with unrealistic infrastructure. Then, a script was used to randomly and uniformly generate 3D models of both small and large wind turbines over the background images, followed by capturing four 608x608 pixel images using a virtual camera. This process was repeated with the same random seed, but with no background image and the wind turbines colored in black. The black and white images were subsequently converted into ground truth labels by grouping the black pixels in the images.